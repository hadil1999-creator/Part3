{
 "Misinterpreting Output": {
    "description": "ML cloud services often provide pre-trained models that return simplified metrics derived from complex internal representations. Developers may misinterpret these outputs if they overlook how multiple values should be combined or misunderstand their intended meaning. Misinterpreting output can lead to incorrect conclusions, faulty application logic, and subtle bugs. For example, Googleâ€™s NLP Sentiment Analysis API returns both a 'score' (sentiment polarity, ranging from -1.0 for negative to +1.0 for positive) and a 'magnitude' (overall sentiment strength, ranging from 0.0 and above). To accurately determine sentiment, code should consider both values together: a positive score with a significant magnitude indicates strong positive sentiment, a negative score with a significant magnitude indicates strong negative sentiment, and a score near zero or with low magnitude indicates neutral sentiment. Ignoring this can result in inaccurate sentiment assessments, reducing model reliability and the quality of decision-making."
},

  "Non Specification Of Early Stopping Criteria": {
    "description": "ML services typically offer options to set early stopping criteria, helping to avoid overfitting and unnecessary computational expenses. However, developers sometimes fail to specify these criteria, which allows the training to proceed for more epochs than necessary. This oversight can result in wasted computational resources, increased training duration, higher costs, and potential overfitting."
  },

   "Ignoring Monitoring Data Drift": {
    "description": "Data drift occurs when the incoming data distribution differs from the training data, leading to degraded model accuracy over time. Cloud providers recommend implementing skew and drift detection mechanisms to monitor these changes and alert developers when significant changes occur. By detecting data drift early, models can be retrained or adjusted to ensure they continue performing as expected in production environments."
  },
  "Not Using Training Checkpoints": {
    "description": "Cloud providers offer the ability to resume training from the most recent checkpoint, saving the current state of the experiment rather than starting from scratch. This can save significant time and computational resources, especially when training large and complex models. However, developers may neglect to save training checkpoints in cloud storage. If a model fails and checkpoints have not been saved, the entire training job or pipeline will terminate, resulting in a loss of data since the model's state is not preserved in memory."
  },
  "Improper Handling of ML API Limits": {
    "description": "Failure to adhere to API request rate limits can compromise the stability and performance of the ML service. Developers may not adequately manage these limits, causing predictions to abruptly halt when the rate is exceeded. For instance, surpassing the maximum number of API calls within a set timeframe, such as requests per second defined by the Azure OpenAI service, can result in delayed or rejected requests until they conform to the permitted rate."
  },
  "Ignoring Testing Schema Mismatch": {
    "description": "Cloud providers offer ML services to detect unmatched data schemas, which include feature or data distribution mismatches between training, testing, and production data, often by raising alerts. However, developers may ignore setting up these alerts or disable them. For example, Amazon ML displays alerts if the schemas for the training and evaluation data sources are not consistent. Disabling these alerts can result in missing discrepancies, such as features present in the training data but absent in the evaluation data, or detecting unexpected additional features. This oversight may weaken the model's accuracy and performance in the production environment."
  },
  "Not Using Batch Api For Data Processing": {
    "description": "Cloud providers offer batch processing APIs to optimize data loading performance by handling data in batches. However, developers sometimes fail to use these batch APIs, choosing instead to load data items individually or implement their own batch processing solutions. Not using a batch API can cause Out-Of-Memory (OOM) issues, excessive network traffic, and significant delays in data loading, ultimately slowing down model training and increasing operational costs."
  }

}
