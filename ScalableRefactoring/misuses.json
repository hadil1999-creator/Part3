{
 "Misinterpreting Output": {
    "description": "ML cloud services (including those from Google Cloud, AWS, and Microsoft Azure) frequently provide pre-trained models that return simplified metrics or aggregated values derived from complex internal representations. Developers may misinterpret these outputs if they overlook how multiple values should be combined or misunderstand the intended meaning of individual metrics. Misinterpreting output can lead to incorrect conclusions, faulty application logic, and subtle, hard-to-trace bugs. For example, many services, such as Google's NLP Sentiment Analysis, return a pair of values like 'score' (polarity, e.g., negative to positive) and 'magnitude' (overall strength or intensity). To accurately determine the final result, developers must consider both values together: a score's direction combined with a significant magnitude indicates a strong, clear classification (e.g., strongly positive or strongly negative), while a score near zero or with low magnitude indicates a neutral or mixed/ambiguous result. Similar multi-metric dependencies exist across various models and providers, such as object detection confidence scores paired with bounding box probabilities, or prediction classes accompanied by loss/error indicators. Ignoring this necessary synthesis reduces model reliability and the quality of decision-making."
  },

  "Non Specification Of Early Stopping Criteria": {
    "description": "ML services typically offer options to set early stopping criteria, helping to avoid overfitting and unnecessary computational expenses. However, developers sometimes fail to specify these criteria, which allows the training to proceed for more epochs than necessary. This oversight can result in wasted computational resources, increased training duration, higher costs, and potential overfitting."
  },

   "Ignoring Monitoring Data Drift": {
    "description": "Data drift occurs when the incoming data distribution differs from the training data, leading to degraded model accuracy over time. Cloud providers recommend implementing skew and drift detection mechanisms to monitor these changes and alert developers when significant changes occur. By detecting data drift early, models can be retrained or adjusted to ensure they continue performing as expected in production environments."
  },
  "Not Using Training Checkpoints": {
    "description": "Cloud providers offer the ability to resume training from the most recent checkpoint, saving the current state of the experiment rather than starting from scratch. This can save significant time and computational resources, especially when training large and complex models. However, developers may neglect to save training checkpoints in cloud storage. If a model fails and checkpoints have not been saved, the entire training job or pipeline will terminate, resulting in a loss of data since the model's state is not preserved in memory."
  },
  "Improper Handling Of Ml Api Limits": {
    "description": "Failure to adhere to API request rate limits can compromise the stability and performance of the ML service. Developers may not adequately manage these limits, causing predictions to abruptly halt when the rate is exceeded. For instance, surpassing the maximum number of API calls within a set timeframe, such as requests per second defined by the Azure OpenAI service, can result in delayed or rejected requests until they conform to the permitted rate."
  },
  "Ignoring Testing Schema Mismatch": {
    "description": "Cloud providers offer ML services to detect unmatched data schemas, which include feature or data distribution mismatches between training, testing, and production data, often by raising alerts. However, developers may ignore setting up these alerts or disable them. For example, Amazon ML displays alerts if the schemas for the training and evaluation data sources are not consistent. Disabling these alerts can result in missing discrepancies, such as features present in the training data but absent in the evaluation data, or detecting unexpected additional features. This oversight may weaken the model's accuracy and performance in the production environment."
  },
  "Not Using Batch Api For Data Processing": {
    "description": "Cloud providers offer batch processing APIs to optimize data loading performance by handling data in batches. However, developers sometimes fail to use these batch APIs, choosing instead to load data items individually or implement their own batch processing solutions. Not using a batch API can cause Out-Of-Memory (OOM) issues, excessive network traffic, and significant delays in data loading, ultimately slowing down model training and increasing operational costs."
  }

}
